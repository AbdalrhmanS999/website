---
title: "Multiple Logistic Regression"
author: ""
output:
  xaringan::moon_reader:
    css: "slides.css"
    lib_dir: libs
    nature:
      highlightLines: true
      highlightStyle: github
      countIncrementalSlides: false
---

```{r child = "setup.Rmd"}
```
layout: true

<div class="my-footer">
<span>
by Dr. Lucy D'Agostino McGowan
</span>
</div> 

---

```{r, message = FALSE, warning = FALSE, echo = FALSE}
library(tidyverse)
library(Stat2Data)
library(broom)
library(knitr)
options(digits = 3)
```

---

## Types of statistical models

response | predictor(s) | model
---------|--------------|------
quantitative | one quantitative | simple linear regression
quantitative | two or more (of either kind) | multiple linear regression
binary | one (of either kind) | simple logistic regression
binary | two or more (of either kind) | multiple logistic regression

---

## Types of statistical models

response | predictor(s) | model
---------|--------------|------
quantitative | one quantitative | simple linear regression
quantitative | two or more (of either kind) | multiple linear regression
binary | one (of either kind) | simple logistic regression
**binary** | **two or more (of either kind)** | **multiple logistic regression**

---

## Types of statistical models

variables | predictor | ordinary regression | logistic regression
---|---|---|---
one: $x$ | $\beta_0 + \beta_1 x$ | Response $y$ | $\textrm{logit}(\pi)=\log\left(\frac{\pi}{1-\pi}\right)$
several: $x_1,x_2,\dots,x_k$| $\beta_0 + \beta_1x_1 + \dots+\beta_kx_k$|Response $y$ | $\textrm{logit}(\pi)=\log\left(\frac{\pi}{1-\pi}\right)$

---

## Multiple logistic regression

* `r emo::ji("v")` forms

Form | Model
-----|------
Logit form | $\log\left(\frac{\pi}{1-\pi}\right) = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots \beta_kx_k$
Probability form | $\Large\pi = \frac{e^{\beta_0 + \beta_1x_1 + \beta_2x_2 + \dots \beta_kx_k}}{1+e^{\beta_0 + \beta_1x_1 + \beta_2x_2 + \dots \beta_kx_k}}$

---

# Steps for modeling

![](img/03/flowchart-arrow.png)

---

## Fit

.small[
```{r}
data(MedGPA)
glm(Acceptance ~ MCAT + GPA, data = MedGPA, family = "binomial") %>%
  tidy(conf.int = TRUE)
```
]
---

## Fit

.question[
What does this do?
]

.small[
```{r}
glm(Acceptance ~ MCAT + GPA, data = MedGPA, family = "binomial") %>% #<<
  tidy(conf.int = TRUE)
```
]

---

## Fit

.question[
What does this do?
]

.small[
```{r}
glm(Acceptance ~ MCAT + GPA, data = MedGPA, family = "binomial") %>%
  tidy(conf.int = TRUE)  #<<
```
]

---

## Fit

.question[
What does this do?
]
.small[
```{r, eval = FALSE}
glm(Acceptance ~ MCAT + GPA, data = MedGPA, family = "binomial") %>%
  tidy(conf.int = TRUE) %>%
  kable() #<<
```
]

.small[
```{r, echo = FALSE}
glm(Acceptance ~ MCAT + GPA, data = MedGPA, family = "binomial") %>%
  tidy(conf.int = TRUE) %>%
  kable(format = "html")
```
]

---

## Assess

.question[
What are the assumptions of multiple logistic regression?
]

--

* Linearity
* Independence
* Randomness
---


## Assess

.question[
How do you determine whether the conditions are met?
]

* Linearity
* Independence
* Randomness

---

## Assess

.question[
How do you determine whether the conditions are met?
]

* Linearity: empirical logit plots
* Independence: look at the data generation process
* Randomness: look at the data generation process (does the spinner model make sense?)

---

## Assess

.question[
If I have two nested models, how do you think I can determine if the full model is significantly better than the reduced?
]

--

* We can compare values of $-2 \log(\mathcal{L})$ (deviance) between the two models

--
* Calculate the "drop in deviance" the difference between  $(-2 \log(\mathcal{L}_{reduced})) - ( -2 \log(\mathcal{L}_{full}))$

--
* This is a "likelihood ratio test"

--
* This is $\chi^2$ distributed with $p$ degrees of freedom

--
* $p$ is the difference in number of predictors between the full and reduced models

---

## Use

* We can calculate confidence intervals for the $\beta$ coefficients: $\hat\beta\pm z^*\times SE_{\hat\beta}$
* To determine whether individual explanatory variables are statistically significant, we can calculate p-values based on the $z$-statistic of the $\beta$ coefficients (using the normal distribution)

---

## Use

.question[
How do you interpret these $\beta$ coefficients?
]

.small[
```{r}
glm(Acceptance ~ MCAT + GPA, data = MedGPA, family = "binomial") %>%
  tidy(conf.int = TRUE)
```
]

---

class: middle

## $\hat\beta$ interpretation in multiple logistic regression

The coefficient for $x$ is $\hat\beta$ (95% CI: $LB_\hat\beta, UB_\hat\beta$). A one-unit increase in $x$ yields a $\hat\beta$ expected change in the log odds of y, **holding all other variables constant**.

---

class: middle

## $e^{\hat\beta}$ interpretation in multiple logistic regression

The odds ratio for $x$ is $e^{\hat\beta}$ (95% CI: $e^{LB_\hat\beta}, e^{UB_\hat\beta}$). A one-unit increase in $x$ yields a $e^{\hat\beta}$-fold expected change in the odds of y, **holding all other variables constant**.


---

## Summary 

 | Ordinary regression | Logistic regression
---|------|----
test or interval for $\beta$ | $t = \frac{\hat\beta}{SE_{\hat\beta}}$ |  $z = \frac{\hat\beta}{SE_{\hat\beta}}$ | 
| t-distribution | z-distribution
test for nested models | $F = \frac{\Delta SSModel / p}{SSE_{full} / (n - k - 1)}$ | G = $\Delta(-2\log\mathcal{L})$|
| F-distribution | $\chi^2$-distribution

