<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Assessing Assumptions: Independence and Randomness</title>
    <meta charset="utf-8" />
    <meta name="author" content="" />
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <link rel="stylesheet" href="slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Assessing Assumptions: Independence and Randomness

---




layout: true

&lt;div class="my-footer"&gt;
&lt;span&gt;
by Dr. Lucy D'Agostino McGowan
&lt;/span&gt;
&lt;/div&gt; 

---



# Steps for modeling

![](img/03/flowchart-arrow.png)

---

## Conditions for ordinary linear regression

* Linearity
* Zero Mean
* Constant Variance
* Independence
* Random
* Normality

---

## Conditions for ordinary linear regression

.small[
Assumption| What it means | How do you check? | How do you fix?
----|-----|-----------|-----
Linearity |The relationship between the outcome and explanatory variable or predictor is linear **holding all other variables constant**| Residuals vs. fits plot | fit a better model (transformations, polynomial terms, more / different variables, etc.)
Zero Mean | | |
]
---

## Conditions for ordinary linear regression


.small[
Assumption| What it means | How do you check? | How do you fix?
----|-----|-----------|-----
Linearity |The relationship between the outcome and explanatory variable or predictor is linear **holding all other variables constant**| Residuals vs. fits plot | fit a better model (transformations, polynomial terms, more / different variables, etc.)
Zero Mean | The error distribution is centered at zero | by default | --
Constant Variance | | |
]

---

## Conditions for ordinary linear regression

.small[
Assumption| What it means | How do you check? | How do you fix?
----|-----|-----------|-----
Linearity |The relationship between the outcome and explanatory variable or predictor is linear **holding all other variables constant**| Residuals vs. fits plot | fit a better model (transformations, polynomial terms, more / different variables, etc.)
Zero Mean | The error distribution is centered at zero | by default | --
Constant Variance | The variability in the errors is the same for all values of the predictor variable | Residuals vs fits plot | fit a better model
Independence | | |
]

---

## Conditions for ordinary linear regression


.small[
Assumption| What it means | How do you check? | How do you fix?
----|-----|-----------|-----
Linearity |The relationship between the outcome and explanatory variable or predictor is linear **holding all other variables constant**| Residuals vs. fits plot | fit a better model (transformations, polynomial terms, more / different variables, etc.)
Zero Mean | The error distribution is centered at zero | by default | --
Constant Variance | The variability in the errors is the same for all values of the predictor variable | Residuals vs fits plot | fit a better model
Independence | The errors are assumed to be independent from one another | üëÄ data generation | Find better data or fit a fancier model
Random | | |

]

---

## Conditions for ordinary linear regression

.small[
Assumption| What it means | How do you check? | How do you fix?
----|-----|-----------|-----
Linearity |The relationship between the outcome and explanatory variable or predictor is linear **holding all other variables constant**| Residuals vs. fits plot | fit a better model (transformations, polynomial terms, more / different variables, etc.)
Zero Mean | The error distribution is centered at zero | by default | --
Constant Variance | The variability in the errors is the same for all values of the predictor variable | Residuals vs fits plot | fit a better model
Independence | The errors are assumed to be independent from one another | üëÄ data generation | Find better data or fit a fancier model
Random | The data are obtained using a random process | üëÄ data generation | Find better data or fit a fancier model
Normality | | |
]

---

## Conditions for ordinary linear regression

.small[
Assumption| What it means | How do you check? | How do you fix?
----|-----|-----------|-----
Linearity |The relationship between the outcome and explanatory variable or predictor is linear **holding all other variables constant**| Residuals vs. fits plot | fit a better model (transformations, polynomial terms, more / different variables, etc.)
Zero Mean | The error distribution is centered at zero | by default | --
Constant Variance | The variability in the errors is the same for all values of the predictor variable | Residuals vs fits plot | fit a better model
Independence | The errors are assumed to be independent from one another | üëÄ data generation | Find better data or fit a fancier model
Random | The data are obtained using a random process | üëÄ data generation | Find better data or fit a fancier model
Normality | The random errors follow a normal distribution | QQ-plot / residual histogram | fit a better model
]

---

## Conditions for **logistic** regression

* Linearity
* Independence
* Randomness

---

## Conditions for **logistic** regression

.question[
How do we test linearity?
]
* **Linearity**
* Independence
* Randomness

---

## Conditions for **logistic** regression

.question[
How do we test linearity?
]
* **Linearity** ‚úÖ _Plot the empirical logits_
* Independence
* Randomness

---

## ‚õ≥ Testing for linearity in logistic regression 

* We can plot the **empirical logit** to examine the linearity assumption

Length | 3 | 4 | 5 | 6 | 7
------|----|---|---|---|---
Number of successes | 84 | 88 | 61 | 61 | 44 
Number of failures | 17 | 31 | 47 | 64 | 90 
Total | 101 | 119 | 108 | 125 | 134
Probability of success | 0.832 | 0.739 | 0.565 | 0.488 | 0.328
Odds of success | 4.941 | 2.839 | 1.298| 0.953| 0.489
Empirical logit | 1.60 | 1.04| 0.26 |‚àí0.05 |‚àí0.72

---

## ‚õ≥ Testing for linearity in logistic regression 

.small[

```r
data &lt;- data.frame(
  length = c(3, 4, 5, 6, 7),
  emp_logit = c(1.6, 1.04, 0.26, -0.05, -0.72)
)
ggplot(data, aes(length, emp_logit)) + 
  geom_point() + 
  geom_abline(intercept = 3.26, slope = -0.566, lty = 2) +
  labs(y = "log odds of success")
```

![](13-independence-random_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;
]

---


# Conditions for **logistic** regression

* Linearity
* **Independence**
* **Randomness**

---

## Why do we care?

![](img/13/flowchart-no-summary.jpeg)

---

## Why do we care?

![](img/13/flowchart-descriptive.jpg)

---

## Why do we care?

![](img/13/flowchart-exploratory.jpg)

---

## Why do we care?

![](img/13/flowchart-care.jpg)

---

## Independence

* The observations should be _independent_ of one another
* In the Medical School Admission - GPA example, 55 students were randomly selected and their admission status and GPA were recorded: üëç Independent or üëé Not?

--
* üëç

--
* A new treatment comes out to help with dry eyes. A sample of 25 people get randomized to either treatment or placebo - this creates a dataset of information about 50 eyes. üëç or üëé 

--
* üëé

---

## Randomness

.question[
What do I put for the **family** argument if I want to fit a logistic regression in R?
]


```r
glm(am ~ mpg, data = mtcars,
*   family = "---")
```

---

## Randomness

.question[
What do I put for the **family** argument if I want to fit a logistic regression in R?
]


```r
glm(am ~ mpg, data = mtcars,
*   family = "binomial")
```

* The **binomial** distribution tells you the number of successes in `\(n\)` experiments

--
* For the purposes of this class, `\(n\)` will always be 1

--
* This is special case of the **binomial** distribution, called the **Bernoulli** distribution

--
* Why does this matter? R wants you to specify that you are using the `"binomial"` family, your book talks about the **Bernoulli** distribution, I just want to bridge the gap

---

## Randomness

_The Spinner_

* A random 0, 1 outcome that behaves like a "spinner" follows the **Bernoulli** distribution

--
* For example, in a fair coin toss, the probability of getting "heads" is 50%

![](13-independence-random_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;

---

## Randomness

_The Spinner_

* A random 0, 1 outcome that behaves like a "spinner" follows the **Bernoulli** distribution
* For example, in a fair coin toss, the probability of getting "heads" is 50%

![](13-independence-random_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;

---

## Randomness

_The Spinner_

* A random 0, 1 outcome that behaves like a "spinner" follows the **Bernoulli** distribution
* For example, in a fair coin toss, the probability of getting "heads" is 50%

![](13-independence-random_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;

---

## Randomness

_The Spinner_

* A random 0, 1 outcome that behaves like a "spinner" follows the **Bernoulli** distribution
* For example, in a fair coin toss, the probability of getting "heads" is 50%

![](13-independence-random_files/figure-html/unnamed-chunk-9-1.png)&lt;!-- --&gt;

---

## Randomness

_The Spinner_

* A random 0, 1 outcome that behaves like a "spinner" follows the **Bernoulli** distribution
* In the Medical school admissions - GPA example, a student with a 3.8 has an 82% chance of acceptance

![](13-independence-random_files/figure-html/unnamed-chunk-10-1.png)&lt;!-- --&gt;

---


## Randomness

_The Spinner_

* A random 0, 1 outcome that behaves like a "spinner" follows the **Bernoulli** distribution
* In the Medical school admissions - GPA example, a student with a 3.8 has an 82% chance of acceptance

![](13-independence-random_files/figure-html/unnamed-chunk-11-1.png)&lt;!-- --&gt;

---

## Randomness

_The Spinner_

* A random 0, 1 outcome that behaves like a "spinner" follows the **Bernoulli** distribution
* In the Medical school admissions - GPA example, a student with a 3.8 has an 82% chance of acceptance

![](13-independence-random_files/figure-html/unnamed-chunk-12-1.png)&lt;!-- --&gt;

---

## Randomness

_The Spinner_

* A random 0, 1 outcome that behaves like a "spinner" follows the **Bernoulli** distribution
* In the Medical school admissions - GPA example, a student with a 3.8 has an 82% chance of acceptance

![](13-independence-random_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;

---

## Randomness

_The Spinner_

* A random 0, 1 outcome that behaves like a "spinner" follows the **Bernoulli** distribution
* In the Medical school admissions - GPA example, a student with a 3.8 has an 82% chance of acceptance

![](13-independence-random_files/figure-html/unnamed-chunk-14-1.png)&lt;!-- --&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true,
"highlightStyle": "github",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
